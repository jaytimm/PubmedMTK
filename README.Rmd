---
output:
  md_document:
    variant: markdown_github
---


# PubMed Mining Toolkit - an overview

A collection of functions and resources for accessing, manipulating, and visualizing data made available on PubMed using R, including (1) PubMed query tools, (2) query exploration tools, and (3) abstract-level data acquisition tools.  

Included here is a set of example applications, including: 

* Automated, independent searches for multiple (& complex) search queries;
* More in-depth exploration of PubMed search results; namely, the investigation of high-level co-occurrence associations among query terms;
* Extraction of abstract-level metadata for (1) comparing citation trends historically and (2) investigating annotation-based topic structure within a set abstracts; and
* Creation of custom text corpora based on PubMed abstracts.


---

```{r eval=FALSE, include=FALSE}
usethis::use_package('rentrez')
usethis::use_package('RCurl')
usethis::use_package('xml2')
usethis::use_package('data.table')
usethis::use_package('XML')
usethis::use_package('Matrix')
```



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("/home/jtimm/jt_work/GitHub/packages/render_toc.R")

#Warning message:
# replacing previous import ‘data.table::melt’ by ‘reshape2::melt’ when loading ‘PubmedMTK’
```


```{r echo=FALSE}
render_toc("/home/jtimm/jt_work/GitHub/PubmedMTK/README.Rmd",
           toc_header_name = "PubMed Mining Toolkit - an overview",
           toc_depth = 2)
```


## Installation

```{r eval=FALSE}
devtools::install_github("jaytimm/PubmedMTK")
```


## Usage

```{r message=FALSE, warning=FALSE}
working_dir <- '/home/jtimm/jt_work/GitHub/PubmedMTK/data-raw/'

if (!require("pacman")) install.packages("pacman")
pacman::p_load(data.table, # quanteda, 
               rentrez, 
               XML, xml2, RCurl,
               reshape2, #text2vec,  
               tokenizers, 
               tm,
               tidytext,
               Matrix.utils,
               janitor,
               ggplot2, knitr,
               magrittr, dplyr, tidyr)
```


```{r message=FALSE, warning=FALSE}
# Set NCBI API key
# ncbi_key <- '4f47f85a9cc03c4031b3dc274c2840b06108'
# rentrez::set_entrez_key(ncbi_key)
```




### MeSH vocabulary 

For starters, the package includes as a data frame the MeSH thesaurus & hierarchically-organized vocabulary -- comprised of 2021 versions of `descriptor` & `trees` files made available via NLM-NIH.  [A workflow for re-creating the table from raw data sets](https://github.com/jaytimm/PubmedMTK/blob/main/build-MeSH-df.md).  

```{r eval=FALSE, include=FALSE}
Also, [a useful reference](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5324252/).
```


```{r message=FALSE, warning=FALSE}
knitr::kable(head(PubmedMTK::pmtk_tbl_mesh))
```



### PubMed search - `pmtk_search_pubmed()`

The `pmtk_search_pubmed()` function is meant for record-matching searches typically performed using the [PubMed online interface](https://pubmed.ncbi.nlm.nih.gov/).  If multiple search terms are specified, independent queries are performed per term. Output includes record IDs per search term -- which can subsequently be used to fetch full records/abstracts.  

Search terms are by default translated into NCBI syntax; for simplicity, search is focused on *MeSH headings* ([MH]) and *titles & abstracts* ([TIAB]).  So: a search for `aging` is translated as `aging[MH] OR aging[TIAB]`.  The user can specify their own PubMed-ready queries by setting the `translate_syntax` parameter to FALSE.

```{r message=FALSE, warning=FALSE}
pmed_search <- c('human life span',
                 'senescence', 
                 'proteostasis',
                 'dna damage', 
                 'beta galactosidase', 
                 'genomic instability',
                 'telomere attrition',
                 'epigenetic alterations',
                 'mitochondrial dysfunction',
                 'cellular senescence',
                 'stem cell exhaustion')
```


```{r eval=FALSE}
search_results1 <- PubmedMTK::pmtk_search_pubmed(pmed_search = pmed_search)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
setwd(working_dir)
#saveRDS(search_results1, 'search_results1.rds')
search_results1 <- readRDS('search_results1.rds')
```



#### Summary of record counts returned by PubMed query

```{r message=FALSE, warning=FALSE}
# ## Total citations per search term are summarized below:
search_results1 %>%
  group_by(search) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  #janitor::adorn_totals() %>%
  knitr::kable()
```


#### Unique records

```{r}
length(unique(search_results1$pmid))
```



### Advanced counting - `pmtk_crosstab_query()`

Based simply on the table of record IDs returned from `pmtk_search_pubmed()`, we can quickly investigate co-occurrence among our set of search terms.  Here, `term1` and `term2` are said to co-occur in *abstract X* if independent PubMed queries for `term1` and `term2` both return *abstract X*.  Ideal for quick exploration.  

The table below details co-occurrence patterns among `senescence`-based search pairs. Columns `n1` and `n2` specify citation counts for `term1` and `term2`, respectively; column `n1n2` specifies the count of citations returned by both `term1` & `term2`, ie, the intersect between the two sets of record IDs.  The strength of association between `term1` & `term2`, then, is approximated via pointwise mutual information (`pmi`), which is computed as the log difference of actual vs. expected co-occurrence probabilities.

```{r}
search_tab <- PubmedMTK::pmtk_crosstab_query(search_results1) %>%
  mutate(pmi = round(log( (n1n2/1e6) / ( (n1/1e6) * (n2/1e6) )), 3))

search_tab %>% filter(term1 == 'senescence') %>% knitr::kable()
```



**The plot below** details PMI-based associative strengths among search terms (as ~heatmap).  Labels specify co-occurrence in terms of citation counts; color denotes PMI value.

```{r fig.height=6, message=FALSE, warning=FALSE}
search_tab %>%
  mutate(pmi = ifelse(pmi < 0, 0, pmi),
         term2 = as.character(term2)) %>%
  rowwise() %>%
  
  mutate(ords = paste(sort(c(term1, term2)), collapse = '_')) %>%
  group_by(ords) %>% slice(1) %>% ungroup() %>%
  
  #arrange(term1, term2) %>%
  ggplot(aes(x = term1, y = term2, fill = pmi)) + 
  geom_tile() + 
  geom_text(aes(fill = pmi, label = n1n2), size = 3) + 
  scale_fill_gradient2(low = scales::muted("#d8b365"), 
                       mid = "#f5f5f5", 
                       high = scales::muted('#5ab4ac'),
                       midpoint = 0) +
  theme_minimal() + xlab('') + ylab('') +
  theme(legend.position = 'none',
        axis.text.x = element_text(angle = 45, hjust = 1))
```




### Fetch abstract data from PubMed

As a two-step process: (1) `pmtk_download_abs()` and (2) `pmtk_loadr_abs()`.  Other R packages provide access to NCBI-PubMed data via the `Eutils` API (most notable being `rentrez`); however, none of them are perfectly suited for fetching PubMed abstracts in bulk, or building text corpora.  

**The approach utilized here** is not the most elegant, but it makes the most out of rate-limits by utilizing a combination of local storage and "more + smaller" API batch queries.  Each "batch" contains n = 199 records; batch files are converted from XML to a data frame in RDS format and stored locally in a user-specified file path.   



#### Download batch data - `pmtk_download_abs()` 

The `out_file` parameter specifies the file path for local batch file storage; the `file_prefix` parameter specifies a character string used to identify batches (along with a batch #). 

```{r eval=FALSE, message=FALSE, warning=FALSE}
PubmedMTK::pmtk_download_abs(pmids = unique(search_results1$pmid),
                             out_file = paste0(working_dir, 'batches/'),
                             file_prefix = 'sen')
```



#### Load batch data - `pmtk_loadr_abs()`

The `pmtk_loadr_abs()` function loads batch files as two data frames: the first, a corpus object containing the record id and abstract, and the second, a metadata object including record id and all other record details, eg, article name, MeSH terms, Pub Date, etc.

```{r message=FALSE, warning=FALSE}
# batch_dir <- dr
batch_dir <- paste0(working_dir, 'batches/')
sen_df <- PubmedMTK::pmtk_loadr_abs(in_file = batch_dir, 
                                    file_prefix = 'sen')
```



#### CORPUS details 

Some descriptives for the resulting corpus of abstracts are detailed below:   

```{r}
sen_df$tif %>%
  mutate(includes_abstract = ifelse(is.na(text), 'N', 'Y')) %>%
  count(includes_abstract) %>%
  mutate(tokens = ifelse(includes_abstract == 'Y',
                         n* 210, NA)) %>%
  knitr::kable()
```



```{r}
sen_df$tif$text[1191] %>% strwrap()
```




#### RECORD details 

Each PubMed record returned from batch download includes some or all of the following attributes:

```{r}
sen_df$meta %>%
  filter(complete.cases(.)) %>% 
  slice(1) %>%
  data.table::transpose(keep.names = "var") %>%
  mutate(V1 = gsub('\\|', ' \\| ', V1)) %>%
  knitr::kable()
```




### Citation trends historically

Based on (1) search results from the `pubmed_get_ids` function, and (2) the metadata returned from `pmtk_loadr_abs()`, we can investigate & compare historical citation frequencies for our set of search terms.  The package ships with a table summarizing [total Medline citation counts by year](https://www.nlm.nih.gov/bsd/medline_cit_counts_yr_pub.html), which facilitates straightforward computation of relative citation frequency for search term(s) by year (here, per 100K citations). 


```{r}
## in theory, this could be used for other things -- 
tr <- subset(sen_df$meta, !grepl('[a-z]', year))
tr$year <- as.Date(paste(tr$year, '01', '01', sep = '-'))
tr1 <- tr[search_results1, on = 'pmid']

## 
meds <- data.table::data.table(PubmedMTK::pmtk_tbl_citations)
tr2 <-  tr1[, list(n = .N), by = list(search, year)]
tr3 <- subset(tr2, year > as.Date('1969', format = '%Y') &
                year < as.Date('2019', format = '%Y') )
tr4 <- meds[tr3, on = 'year']
tr4$per_100k = round(tr4$n / tr4$total * 100000, 3)
```



#### Citation frequencies (per 100K total citations) for `senescence`related search terms from 1970 to 2018. 

```{r fig.height=6, message=FALSE, warning=FALSE}
## Via ggplot --
tr4 %>%
  ggplot() +
  geom_line(aes(x = year,
                #y = n,  ## RAW Citation counts
                y = per_100k, ## Relative citation counts -- 
                group = search,
                color = search),
            size = 1
            ) +
  theme_minimal() +
  ggthemes::scale_color_stata() +
  theme(legend.position = 'right',
        legend.title = element_blank())  +
  ylab('Per 100,000 Medline citations') +
  ggtitle('Relative frequencies of search term citations historically')
```





### Extract MeSH classifications - `pmtk_gather_mesh()`

Subject terms/headings in metadata table include `MeSH` terms, as well as (some) `keywords` & `chem-names`.   The `pmtk_gather_mesh` function extracts these attributes from metadata, and outputs them in structured format.  The resulting table amounts to a document-term matrix (DTM), in which each PubMed abstract is represented as a vector of MeSH terms.  

```{r eval=FALSE, message=FALSE, warning=FALSE}
## this takes too long --based
meshes <- PubmedMTK::pmtk_gather_mesh(meta_df = sen_df$meta)
txts <- length(unique(meshes$pmid))


## get frequencies -- 
freqs <-  meshes[, list(doc_freq = length(unique(pmid))), 
                 by = list(descriptor_name)]
freqs$doc_prop <- freqs$doc_freq/ txts
freqs1 <- subset(freqs, doc_prop > 0.0001 & doc_prop < 0.02)

meshes1 <- subset(meshes, descriptor_name %in% freqs1$descriptor_name)
meshes1 <- subset(meshes1, nchar(descriptor_name) > 0)
```



```{r include=FALSE}
setwd(working_dir)
meshes1 <- readRDS('meshes1.rds')
#saveRDS(meshes1, 'meshes1.rds')
```




Vector representations for a sample set of PubMed records is detailed below: 

```{r message=FALSE, warning=FALSE}
set.seed(999)
meshes1 %>%
  filter(pmid %in% sample(unique(meshes1$pmid), 5)) %>%
  group_by(pmid) %>%
  summarize (mesh_reps = paste0(descriptor_name, collapse = ' | ')) %>%
  knitr::kable()
```




### Subject-based lexicon

Reference corpora, key-ness, and the [PMC Open Access Subset](https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/).  Forthcoming.



### MeSH-based topic model

We then use these MeSH-based abstract representations to explore the conceptual/semantic structure of abstracts included in the `senescence` corpus via topic modeling.

> Latent Dirichlet allocation: a topic modeling algorithm that models **each document** in corpus as a composite of topics, and **each topic** as a composite of terms.  

Topic composition can be interpreted as sets of MeSH terms that frequently co-occur.


```{r eval=FALSE, message=FALSE, warning=FALSE}
mesh_dtm <- tidytext::cast_sparse(data = meshes1,
                                  row = pmid,
                                  column = descriptor_name,
                                  value = count)

mesh_lda <- text2vec::LDA$new(n_topics = 30) ## This is the model
topic_model_fit <- mesh_lda$fit_transform(mesh_dtm, progressbar = F)
```



```{r include=FALSE}
setwd(working_dir)
#saveRDS(mesh_lda, 'mesh_lda.rds')
mesh_lda <- readRDS('mesh_lda.rds')
```



The `mtk_summarize_lda` function summarizes and extracts topic composition from the `text2vec::LDA` output. For each possible topic-feature pair, the model computes the likelihood a given topic generated a given feature.  Output is filtered to the highest scoring features per topic using the `topic_feats_n`.

```{r message=FALSE, warning=FALSE}
tm_summary <- PubmedMTK::mtk_summarize_lda(
  lda = mesh_lda, topic_feats_n = 15)
```



#### Feature composition of first ten topics

```{r echo=FALSE}
tm_summary$topic_summary %>% slice(1:10) %>% knitr::kable()
```



<br>

---

### Topic model summary - html widget

An interactive html widget for exploration of topic model results, and underlying conceptual structure. Presently only view-able via RStudio; a more generic solution in progress.

```{r eval=FALSE}
## topic model html widget
#mesh_lda$plot()
mesh_lda$plot(out.dir = "ldavis", open.browser = FALSE)
```

![](README_files/figure-markdown_github/demo-tm-viz.png)


<br>


---

### Google image analysis

Lastly, some fairly simple functionality (not presented in much detail here) for building collages based on a Google image search.  Below, results from a Google Image search for `human senescence` -- 


![](README_files/figure-markdown_github/summary.png)




